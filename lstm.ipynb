{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Order Invariant Sequences Using LSTMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Purpose:** \n",
    "The purpose of this program is to perform meaningful calculations on a sequence of data using Long Short Term Memory. This program particularly takes in sets of 10 images that contain hand written digits and tries to count how many of each digit there are. This is a variation on a simple classification problem. This code is a simple implementation of the paper ORDERMATTERS: SEQUENCE TO SEQUENCE FOR SETS (Vinyals et al., 2015b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup:** \n",
    "A requirement for this program to work is to have the mnist training data in the same folder as this code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure parameters for this network:\n",
    "`\n",
    "#HyperParameters\n",
    "\n",
    "seq_batch_size = 3\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "num_cycles=2000000\n",
    "\n",
    "learning_rate = 0.0015\n",
    "\n",
    "stride = 1\n",
    "\n",
    "\n",
    "#Parameters\n",
    "\n",
    "\n",
    "layer1_output_nodes = 32\n",
    "\n",
    "layer2_output_nodes = 64\n",
    "\n",
    "dense_layer1_output_nodes = 64\n",
    "\n",
    "dense_layer2_output_nodes = 128\n",
    "\n",
    "img_width = 28\n",
    "\n",
    "img_height = 28\n",
    "\n",
    "img_color_dim = 1\n",
    "\n",
    "seq_length=10\n",
    "\n",
    "T_steps=5\n",
    "\n",
    "lamda=.001\n",
    "\n",
    "mem_size=64\n",
    "\n",
    "save_rate=5000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, img_width, img_height, img_color_dim])\n",
    "X_seq = tf.placeholder(tf.float32, [None])\n",
    "X_Train, Y_Train=data.import_mnist()\n",
    "Qt=tf.placeholder(tf.float32, [None])\n",
    "Ct=tf.placeholder(tf.float32, [None])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Create Embedding network which is just a CNN\n",
    "    \n",
    "`def conv_net(X, mem_size,reuse=True):\n",
    "    with tf.variable_scope(\"Convnet\", reuse=tf.AUTO_REUSE):\n",
    "        layer1 = nnUtils.create_new_conv_layer(X, layer1_output_nodes, [5, 5], [1, 1], stride, \"layer1\")\n",
    "        layer2 = nnUtils.create_new_conv_layer(layer1, layer2_output_nodes, [5, 5], [1, 1], stride, \"layer2\")\n",
    "        flattened = tf.contrib.layers.flatten(layer2)\n",
    "        dense_layer1 = tf.layers.dense(flattened,128, activation=tf.nn.relu)\n",
    "        dense_layer2=tf.layers.dense(dense_layer1,256, activation=tf.nn.relu)\n",
    "        mem_vector = tf.layers.dense(dense_layer2, mem_size)\n",
    "    return mem_vector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function creates a memory cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_t_LSTM(q_t_p,ct,reuse=False):\n",
    "    with tf.variable_scope(\"Memory/Cell\", reuse=reuse):\n",
    "        ht, ct =nnUtils.lstm(q_t_p,ct,mem_size*2)\n",
    "        qt=tf.layers.dense(ht,mem_size)\n",
    "        return qt, ct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process images\n",
    "#Achieves order invariance by using addition which in communicative\n",
    "\n",
    "`def process_block(T_steps,X,X_seq,m, q_t_p,ct, first_call=False):\n",
    "    for t in range(T_steps):\n",
    "        if first_call:\n",
    "            q_t, ct =q_t_LSTM(q_t_p, ct,reuse=(t!=0))\n",
    "        else:\n",
    "            q_t, ct =q_t_LSTM(q_t_p, ct, reuse=True)\n",
    "        r_t=[]\n",
    "        e_j=[]\n",
    "        x=[]\n",
    "        qtt=tf.transpose(q_t)\n",
    "        e_j=tf.matmul(m,qtt)\n",
    "        a_j=tf.nn.softmax(e_j)\n",
    "        for i in range(seq_length):\n",
    "            r_t.append(a_j[i]*m[i])\n",
    "        r_t=tf.reduce_sum(r_t,0)\n",
    "        for i in range(mem_size):\n",
    "            x.append(q_t[0][i])\n",
    "        for i in range(mem_size):\n",
    "            x.append(r_t[i])\n",
    "        q_t_p=x\n",
    "    return tf.concat([q_t[0],r_t],0), ct`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Output Guess\n",
    "`Performs the calculation for the guesses on how many of each digit there are\n",
    "def write_block(qtp, reuse=True):\n",
    "    with tf.variable_scope(\"Decoder\",reuse=reuse):\n",
    "        x=tf.transpose(tf.expand_dims(tf.expand_dims(tf.expand_dims(qtp, 1), 1),1))\n",
    "        layer1 = nnUtils.create_new_conv_layer(x, layer1_output_nodes, [5, 5], [1, 1], stride, \"layer1\")\n",
    "        flattened = tf.contrib.layers.flatten(layer1)\n",
    "        dense_layer1 = tf.layers.dense(flattened,64, activation=tf.nn.relu)\n",
    "        dense_layer2 = tf.layers.dense(dense_layer1,seq_length)\n",
    "        mem_vector=tf.reshape(dense_layer2,[-1,seq_length])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Calculations and train network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        saver.restore(sess, full_path)\n",
    "        sess.run(increment_iter)\n",
    "        print(\"Loaded Checkpoint!\")\n",
    "    except:\n",
    "        sess.run(init)\n",
    "    writer.add_graph(sess.graph)\n",
    "    total_loss=0\n",
    "    recent_loss=0\n",
    "    summ=tf.summary.merge_all()\n",
    "    qt=[]\n",
    "    ct=[]\n",
    "    out_2=[]\n",
    "    for i in range(num_cycles):\n",
    "        xseqbatch,xbatch=data.generate_variable_sequence(X_Train,Y_Train,seq_length)\n",
    "        if i==0:\n",
    "            q_t_p=np.random.rand(mem_size*2).astype(np.float32)\n",
    "            ct=np.random.rand(mem_size*2).astype(np.float32)\n",
    "            qt, ct= sess.run(Process_init,{X: xbatch, X_seq: xseqbatch, Qt: q_t_p, Ct: ct})\n",
    "            qt=np.transpose(qt)\n",
    "            ct=np.transpose(ct[0])\n",
    "            mem=sess.run(m,{X: xbatch})\n",
    "            out=sess.run(Write_init,{m: mem, Qt: qt, Ct: ct})\n",
    "        else:\n",
    "            qt, ct= sess.run(Process,{X: xbatch, X_seq: xseqbatch, Qt: qt, Ct: ct})\n",
    "            qt=np.transpose(qt)\n",
    "            ct=np.transpose(ct[0])\n",
    "            mem=sess.run(m,{X: xbatch})\n",
    "            out=sess.run(Write,{m: mem, Qt: qt, Ct: ct})\n",
    "        out_2 = point_alloc=get_points(out[0])\n",
    "        #out_2=np.reshape(out_2,[1,-1])\n",
    "        _, loss = sess.run([optimizer, write_loss], {\n",
    "                       Write: out, X_seq: xseqbatch, X: xbatch, Qt: qt, Ct: ct})\n",
    "        total_loss+=loss\n",
    "        recent_loss+=loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Cell block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(ht,ct_old,num_layers):\n",
    "    ht=tf.reshape(ht,[-1,num_layers])\n",
    "    ct_old=tf.reshape(ct_old,[-1,num_layers])\n",
    "    ft=tf.layers.dense(ht, num_layers, activation=tf.nn.sigmoid)\n",
    "    it=tf.layers.dense(ht, num_layers, activation=tf.nn.sigmoid)\n",
    "    c_tilda=tf.layers.dense(ht, num_layers, activation=tf.nn.tanh)\n",
    "    ct=tf.add(tf.multiply(ft,ct_old),tf.multiply(it,c_tilda))\n",
    "    ot=tf.layers.dense(ht, num_layers, activation=tf.nn.sigmoid)\n",
    "    ht_new=ot*tf.nn.tanh(ct)\n",
    "    return ht_new, ct\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
